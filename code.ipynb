{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data processing (first step)\n",
    "'''\n",
    "import pandas as pd\n",
    "#load the raw data\n",
    "data = pd.read_csv('raw.csv')\n",
    "#classify the weibo based on the hour it was released\n",
    "def split_time(x):\n",
    "    hour = x['created_at'].hour\n",
    "    if hour<6:\n",
    "        return 0\n",
    "    if hour<9:\n",
    "        return 1\n",
    "    if hour<12:\n",
    "        return 2\n",
    "    if hour<14:\n",
    "        return 3\n",
    "    if hour<18:\n",
    "        return 4\n",
    "    return 5\n",
    "\n",
    "# quantificate the indexes\n",
    "data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "data['time'] = data.apply(lambda x:split_time(x),axis=1)\n",
    "data['at_num'] = data.apply(lambda x:x['content'].count('@'),axis=1)\n",
    "data['hash_num'] = data.apply(lambda x:x['content'].count('#'),axis=1)\n",
    "data['has_video'] = data.apply(lambda x:0 if pd.isnull(x['video_url']) else 1,axis=1)\n",
    "data['has_image'] = data.apply(lambda x:0 if pd.isnull(x['image_url']) else 1,axis=1)\n",
    "data['is_origin'] = data.apply(lambda x:1 if pd.isnull(x['origin_weibo']) else 0,axis=1)\n",
    "data['year'] = data.apply(lambda x:pd.to_datetime(x['created_at']).year,axis=1)\n",
    "data=data.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "# dimensionality reduction\n",
    "data = data.join(pd.get_dummies(data.time))\n",
    "data = data.join(pd.get_dummies(data.year))\n",
    "data.to_csv('Weibo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cluster of different news type\n",
    "For this part, the result may be different every time clustering was run, so the result may differ from what we used in the report\n",
    "'''\n",
    "\n",
    "import jieba\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import codecs\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#put the content column into a csv file \n",
    "y = data['content']\n",
    "y.to_csv('content.csv')\n",
    "#drop the index of content.csv\n",
    "data = pd.read_csv('content.csv')\n",
    "data.columns = ['index','content']\n",
    "data = data.drop(['index'], axis=1)\n",
    "data.to_csv('content1.csv')\n",
    "\n",
    "def getStopWords():\n",
    "    \"\"\"\n",
    "    creat a list of 'stopwords' \n",
    "    as the base to split the content into list of words\n",
    "    \"\"\"\n",
    "    stopwords = []\n",
    "    #import the txt file\n",
    "    for word in open(\"stopwords.txt\", \"r\",encoding='utf-8'):\n",
    "        stopwords.append(word.strip())\n",
    "    return stopwords\n",
    "\n",
    "def load_articles():\n",
    "    \"\"\"\n",
    "    split the content into words\n",
    "    \"\"\"\n",
    "    #get the content and stopwords\n",
    "    stop_words=getStopWords()\n",
    "    data = pd.read_csv('content1.csv')\n",
    "    #show the process\n",
    "    print(\"clustering now ...\")\n",
    "    #cut the content to words and put words that are not in the stopwords list into one index (with whitespace between) \n",
    "    data['content'] = data['content'].apply(\n",
    "        lambda x: \" \".join([word for word in jieba.cut(str(x)) if word not in stop_words]))\n",
    "    \n",
    "    #put these words (one index for one Weibo) into a list\n",
    "    articles = []\n",
    "    for  content in data['content'].tolist():\n",
    "        article =  content\n",
    "        articles.append(article)\n",
    "    return articles\n",
    "\n",
    "def transform(articles, n_features=1000):\n",
    "    \"\"\"\n",
    "    get the data of tf-idf of each words\n",
    "    \"\"\"\n",
    "    #get the vectorized result of Tfidf of the words in each Weibo\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=n_features, min_df=5, use_idf=True)\n",
    "    X = vectorizer.fit_transform(articles)\n",
    "    return X, vectorizer\n",
    "\n",
    "\n",
    "def train(X, vectorizer, true_k=10, mini_batch=False, show_label=False):\n",
    "    \"\"\"\n",
    "    cluster the data with k-means algrithm\n",
    "    \"\"\"\n",
    "    #MiniBatch K-means algorithm's main idea is to use small random batches of data of a fixed size\n",
    "    #MiniBatch K-means is used when the dataset is large, to reduce the time and memory of programming\n",
    "    if mini_batch:\n",
    "        k_means = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                                  init_size=1000, batch_size=1000, verbose=False)\n",
    "    #when the dataset is not large, just use the K-means algrithm \n",
    "    else:\n",
    "        k_means = KMeans(n_clusters=true_k, init='k-means++', max_iter=300, n_init=1,\n",
    "                         verbose=False)\n",
    "    k_means.fit(X)\n",
    "    \n",
    "    # show the distribution of the classes\n",
    "    def plot1(something):\n",
    "        tsne = TSNE(n_components=2)\n",
    "        decomposition_data = tsne.fit_transform(something)\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in decomposition_data:\n",
    "            x.append(i[0])\n",
    "            y.append(i[1])\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = plt.axes()\n",
    "        plt.scatter(x, y, c=k_means.labels_, marker=\"x\")\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.show()\n",
    "\n",
    "    if show_label: \n",
    "        print(\"Top terms per cluster:\")\n",
    "        #find the key words \n",
    "        order_centroids = k_means.cluster_centers_.argsort()[:, ::-1]\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        for i in range(true_k):\n",
    "            print(\"Cluster %d\" % i, end='')\n",
    "            #show the most important 10 key words\n",
    "            for ind in order_centroids[i, :10]:\n",
    "                print(' %s' % terms[ind], end='')\n",
    "            print()\n",
    "    \n",
    "    #put the results of clustering of each Weibo in the list\n",
    "    result = list(k_means.predict(X))\n",
    "    plot1(X.toarray())\n",
    "    datanew = pd.read_csv('Weibo.csv')\n",
    "    resultpd = pd.Series(list(k_means.predict(X)))\n",
    "    datanew['type'] = resultpd\n",
    "    #join the result of the clustering to the dataframe\n",
    "    datanew = datanew.join(pd.get_dummies(datanew.type))\n",
    "    datanew.to_csv('Weibo&cluster.csv')\n",
    "    #show the result\n",
    "    print('Cluster distribution:')\n",
    "    print(dict([(i, result.count(i)) for i in result]))\n",
    "    #return the purity of k-means algrithm\n",
    "    return -k_means.score(X)\n",
    "\n",
    "\n",
    "def plot_params():\n",
    "    \"\"\"\n",
    "    test and find the best parameter\n",
    "    \"\"\"\n",
    "    #get all words in the content\n",
    "    articles = load_articles()\n",
    "    print(\"%d docments\" % len(articles))\n",
    "    #use tf-idf to show the distance of two contents\n",
    "    X, vectorizer = transform(articles, n_features=500)\n",
    "    true_ks = []\n",
    "    scores = []\n",
    "    #try k from 3 to 80 to find how many clusters should be used based on the purity\n",
    "    for i in range(3, 80, 1):\n",
    "        score = train(X, vectorizer, true_k=i) / len(articles)\n",
    "        true_ks.append(i)\n",
    "        scores.append(score)\n",
    "    #plot a figure to show the trend\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(true_ks, scores, label=\"error\", color=\"red\", linewidth=1)\n",
    "    plt.xlabel(\"n_features\")\n",
    "    plt.ylabel(\"error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def out():\n",
    "    \"\"\"\n",
    "    show the reslut of clustering with the parameter of highest purity\n",
    "    \"\"\"\n",
    "    articles = load_articles()\n",
    "    X, vectorizer = transform(articles, n_features=500)\n",
    "    score = train(X, vectorizer, true_k=10, show_label=True) / len(articles)\n",
    "\n",
    "out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_num', 'repost_num', 'like_num', 'created_at',\n",
       "       'image_url', 'video_url', 'origin_weibo', 'content', 'user_id',\n",
       "       'fans_num', 'vip_level', 'tweets_num', 'time', 'at_num', 'hash_num',\n",
       "       'has_video', 'has_image', 'is_origin', 'year', '0:6', '6:9', '9:12',\n",
       "       '12:14', '14:18', '18:24', '2010', '2011', '2012', '2013', '2014',\n",
       "       '2015', '2016', '2017', '2018', '2019', 'type', '爆炸事故', '地质灾害', '航空事故',\n",
       "       '地震灾害', '刑事案件', '森林火灾', '病毒流感'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make adaption of the clustering by hand\n",
    "dataclu=pd.read_csv('Weibo&cluster.csv')\n",
    "dataclu['爆炸事故']=dataclu['0.1']+dataclu['3.1']\n",
    "dataclu['地质灾害']=dataclu['1.1']\n",
    "dataclu['航空事故']=dataclu['2.1']\n",
    "dataclu['地震灾害']=dataclu['4.1']+dataclu['6']\n",
    "dataclu['刑事案件']=dataclu['5.1']+dataclu['8']\n",
    "dataclu['森林火灾']=dataclu['7']\n",
    "dataclu['病毒流感']=dataclu['9']\n",
    "dataclu=dataclu.drop(['0.1','1.1','2.1','3.1','4.1','5.1','6','7','8','9','Unnamed: 0', 'Unnamed: 0.1'],axis=1)\n",
    "#rename the columns\n",
    "dataclu.rename(columns={'0':'0:6','1':'6:9','2':'9:12','3':'12:14','4':'14:18','5':'18:24'}, inplace = True)\n",
    "\n",
    "\n",
    "#dataclu.to_csv('dataclustered.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multinomial Naive Bayes--predicttion of comment_num,like_num,repost_num\n",
    "'''\n",
    "#set the factors used for prediction\n",
    "import pandas as pd\n",
    "data = pd.read_csv('dataclustered.csv')\n",
    "df=data.drop(['Unnamed: 0','id','comment_num','repost_num','like_num','created_at','image_url','video_url','origin_weibo','content','user_id','time','year'],axis=1)\n",
    "X = df\n",
    "# import the packages needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "list_accuracy=list()\n",
    "#set the lables of comment_num\n",
    "bins = [-1, 50, 100, 1000, 136265]\n",
    "data['comment_num'] = pd.cut(data['comment_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['comment_num']\n",
    "#predict the number of comment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, predicted))\n",
    "\n",
    "#set the lables of like_num\n",
    "bins = [-1,75 , 500, 1000, 306161]\n",
    "data['like_num'] = pd.cut(data['like_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['like_num']\n",
    "#predict the number of likes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, predicted))\n",
    "\n",
    "#set the lables of like_num\n",
    "bins = [-1, 100, 500, 1000,  565454]\n",
    "data['repost_num'] = pd.cut(data['repost_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['repost_num']\n",
    "#predict the number of likes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, predicted))\n",
    "\n",
    "print(list_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "KNN--predicttion of comment_num,like_num,repost_num\n",
    "'''\n",
    "#set the factors used for prediction\n",
    "import pandas as pd\n",
    "data = pd.read_csv('dataclustered.csv')\n",
    "df=data.drop(['Unnamed: 0','id','comment_num','repost_num','like_num','created_at','image_url','video_url','origin_weibo','content','user_id','time','year'],axis=1)\n",
    "X = df\n",
    "# import the packages needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "list_accuracy=list()\n",
    "#set the lables of comment_num\n",
    "bins = [-1, 50, 100, 1000, 136265]\n",
    "data['comment_num'] = pd.cut(data['comment_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['comment_num']\n",
    "# predict the number of conmments\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "classifier = KNeighborsClassifier(n_neighbors=4)\n",
    "classifier.fit(X_train, y_train);\n",
    "y_pred = classifier.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#set the lables of like_num\n",
    "bins = [-1,75 , 500, 1000, 306161]\n",
    "data['like_num'] = pd.cut(data['like_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['like_num']\n",
    "# predict the number of likes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "classifier = KNeighborsClassifier(n_neighbors=4)\n",
    "classifier.fit(X_train, y_train);\n",
    "y_pred = classifier.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#set the lables of repost_num\n",
    "bins = [-1, 100, 500, 1000,  565454]\n",
    "data['repost_num'] = pd.cut(data['repost_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['repost_num']\n",
    "# predict the number of reposts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "classifier = KNeighborsClassifier(n_neighbors=4)\n",
    "classifier.fit(X_train, y_train);\n",
    "y_pred = classifier.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(list_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SVM--predicttion of comment_num,like_num,repost_num\n",
    "'''\n",
    "#set the factors used for prediction\n",
    "import pandas as pd\n",
    "data = pd.read_csv('dataclustered.csv')\n",
    "df=data.drop(['Unnamed: 0','id','comment_num','repost_num','like_num','created_at','image_url','video_url','origin_weibo','content','user_id','time','year'],axis=1)\n",
    "X = df\n",
    "\n",
    "#import the packages needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "list_accuracy=list()\n",
    "#set the lables of comment_num\n",
    "bins = [-1, 50, 100, 1000, 136265]\n",
    "data['comment_num'] = pd.cut(data['comment_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['comment_num']\n",
    "#predict the number of comments\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "svclassifier = SVC(kernel='rbf',gamma='scale')\n",
    "svclassifier.fit(X_train, y_train);\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#set the lables of like_num\n",
    "bins = [-1,75 , 500, 1000, 306161]\n",
    "data['like_num'] = pd.cut(data['like_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['like_num']\n",
    "#predict the number of likes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "svclassifier = SVC(kernel='rbf',gamma='scale')\n",
    "svclassifier.fit(X_train, y_train);\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#set the lables of repost_num\n",
    "bins = [-1, 100, 500, 1000,  565454]\n",
    "data['repost_num'] = pd.cut(data['repost_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['repost_num']\n",
    "#use the SVM package to predict the number of reposts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "svclassifier = SVC(kernel='rbf',gamma='scale')\n",
    "svclassifier.fit(X_train, y_train);\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(list_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7642010625255414, 0.8167143440948099, 0.7754393134450347]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Random Forest--predicttion of comment_num,like_num,repost_num\n",
    "'''\n",
    "#set the factors used for prediction\n",
    "import pandas as pd\n",
    "data = pd.read_csv('dataclustered.csv')\n",
    "df=data.drop(['Unnamed: 0','id','comment_num','repost_num','like_num','created_at','image_url','video_url','origin_weibo','content','user_id','time','year'],axis=1)\n",
    "X = df\n",
    "#import packages needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "list_accuracy=list()\n",
    "#set the lables of comment_num\n",
    "bins = [-1, 50, 100, 1000, 136265]\n",
    "data['comment_num'] = pd.cut(data['comment_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['comment_num']\n",
    "#predict the number of comments\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#set the lables of like_num\n",
    "bins = [-1,75 , 500, 1000, 306161]\n",
    "data['like_num'] = pd.cut(data['like_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['like_num']\n",
    "#predict the number of likes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#set the lables of repost_number\n",
    "bins = [-1, 100, 500, 1000,  565454]\n",
    "data['repost_num'] = pd.cut(data['repost_num'], bins,labels=[1,2,3,4],right=True)\n",
    "y = data['repost_num']\n",
    "#predict the number of likes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "list_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(list_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adjusted Random Forest algorithm\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def train_test_split(df, y, test_size): ##Split the train and test data\n",
    "    df['label'] = y\n",
    "    test_size = round(test_size * len(df))\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    return train_df, test_df\n",
    "\n",
    "def calculate_accuracy(predictions, labels): ##Calculate accuracy of prediction\n",
    "    predictions_correct = (predictions == labels)\n",
    "    accuracy = predictions_correct.mean()\n",
    "    return accuracy\n",
    "\n",
    "def check_purity(data): ##Check whether all the data are in the same class\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes = np.unique(label_column)\n",
    "    if len(unique_classes) == 1: # this statement is satisfied only when the np.unique gives only a type\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def classify_data(data): ##Classify the data according to different class in a factor\n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "    index = counts_unique_classes.argmax() # take the class that appears the most\n",
    "    classification = unique_classes[index]\n",
    "    return classification\n",
    "\n",
    "def calculate_entropy(data): ##Calculate the entropy of a situation\n",
    "    label_column = data[:, -1]\n",
    "    counts = np.unique(label_column, return_counts=True)[1]\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def calculate_overall_entropy(data_below, data_above): ##Add up the total entropy of a node\n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "    overall_entropy = (p_data_below * calculate_entropy(data_below)\n",
    "                       + p_data_above * calculate_entropy(data_above))\n",
    "    return overall_entropy\n",
    "\n",
    "def split_data(data, split_column, split_value): ##Split a node\n",
    "    split_column_values = data[:, split_column]   \n",
    "    # determine whether it belongs to the below or the above using fancy indexing\n",
    "    data_below = data[split_column_values <= split_value]\n",
    "    data_above = data[split_column_values > split_value]\n",
    "    return data_below, data_above\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, df, y, n_trees,n_bootstrap, n_features, dt_max_depth, min_samples):\n",
    "        self.df, self.y, self.n_trees, self.n_bootstrap, self.n_features  = df, y, n_trees, n_bootstrap, n_features\n",
    "        self.dt_max_depth, self.min_samples = dt_max_depth, min_samples\n",
    "        self.train_df, self.test_df = train_test_split(df, y, test_size=0.2)\n",
    "        self.trees, self.forest = self.random_forest_algorithm()\n",
    "\n",
    "    def bootstrapping(self): ## Bagging (bootstrap aggregating)    \n",
    "        # randomly select n_bootstrap number of pieces of data to build a tree later\n",
    "        bootstrap_indices = np.random.randint(low=0, high=len(self.train_df), size=self.n_bootstrap)\n",
    "        df_bootstrapped = self.train_df.iloc[bootstrap_indices]\n",
    "        return df_bootstrapped\n",
    "\n",
    "    def random_forest_algorithm(self): ##Build random forest (list)\n",
    "        trees =[]\n",
    "        forest = []\n",
    "        for i in range(self.n_trees):\n",
    "            df_bootstrapped = self.bootstrapping()\n",
    "            tree = DecisionTree(df_bootstrapped, self.n_features, self.dt_max_depth, self.min_samples,random_subspace=self.n_features)\n",
    "            trees.append(tree)\n",
    "            forest.append(tree.decision_tree_algorithm(tree.df))\n",
    "        return trees, forest\n",
    "\n",
    "    def random_forest_predictions(self): ##Go through all the trees and return the class that appears the most for each piece of data\n",
    "        df_predictions = {}\n",
    "        for i in range(len(self.trees)):\n",
    "            column_name = \"tree_{}\".format(i)\n",
    "            predictions = self.trees[i].decision_tree_predictions(self.test_df, tree=self.forest[i])\n",
    "            df_predictions[column_name] = predictions\n",
    "        df_predictions = pd.DataFrame(df_predictions)\n",
    "        random_forest_predictions = df_predictions.mode(axis=1)[0]\n",
    "        return random_forest_predictions\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, df, n_features, max_depth, min_samples,random_subspace=None):        \n",
    "        self.df, self.n_features, self.max_depth, self.min_samples = df, n_features, max_depth, min_samples\n",
    "        self.random_subspace = random_subspace\n",
    "\n",
    "    def get_potential_splits(self,data): ##Potential splits\n",
    "        potential_splits = {}\n",
    "        column_indices = list(range(data.shape[1] - 1))  # excluding the last column (label)\n",
    "        if self.random_subspace and self.random_subspace <= len(column_indices):\n",
    "            # randomly select k columns from column_indices\n",
    "            column_indices = random.sample(population=column_indices, k=self.random_subspace)\n",
    "        for column_index in column_indices:\n",
    "            values = data[:, column_index]\n",
    "            unique_values = np.unique(values)\n",
    "            potential_splits[column_index] = unique_values\n",
    "        return potential_splits\n",
    "\n",
    "    def determine_best_split(self,data, potential_splits): ##Choose the split with lowest entropy\n",
    "        overall_entropy = 102102\n",
    "        # go through all the potential splits\n",
    "        for column_index in potential_splits:\n",
    "            for value in potential_splits[column_index]:\n",
    "                data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "                current_overall_entropy = calculate_overall_entropy(data_below, data_above)             \n",
    "                # compare total entropy and select\n",
    "                if current_overall_entropy <= overall_entropy:\n",
    "                    overall_entropy = current_overall_entropy\n",
    "                    best_split_column = column_index\n",
    "                    best_split_value = value\n",
    "        return best_split_column, best_split_value\n",
    "\n",
    "    def decision_tree_algorithm(self, df, counter=0): ##Build a decision tree by dictionary\n",
    "        # data preparations\n",
    "        if counter == 0:\n",
    "            global COLUMN_HEADERS\n",
    "            COLUMN_HEADERS = self.df.columns\n",
    "            data = self.df.values\n",
    "        else:\n",
    "            data = df\n",
    "        # all the conditions that indicates the end of splitting a tree\n",
    "        if (check_purity(data)) or (len(data) < self.min_samples) or (counter == self.max_depth):\n",
    "            classification = classify_data(data)\n",
    "            return classification\n",
    "        # recursion\n",
    "        else:\n",
    "            counter += 1 # in case it reaches the man_depth\n",
    "            # helper functions\n",
    "            potential_splits = self.get_potential_splits(data)\n",
    "            split_column, split_value = self.determine_best_split(data, potential_splits)\n",
    "            data_below, data_above = split_data(data, split_column, split_value)\n",
    "            # return if data is already empty\n",
    "            if len(data_below) == 0 or len(data_above) == 0:\n",
    "                classification = classify_data(data)\n",
    "                return classification\n",
    "            # determine question\n",
    "            feature_name = COLUMN_HEADERS[split_column]\n",
    "            # if print the tree, the sturcture would be like this\n",
    "            question = \"{} <- {}\".format(feature_name, split_value)\n",
    "            # instantiate sub-tree\n",
    "            sub_tree = {question: []}\n",
    "            # find answers (recursion)\n",
    "            yes_answer = self.decision_tree_algorithm(data_below, counter)\n",
    "            no_answer = self.decision_tree_algorithm(data_above, counter)\n",
    "            # If the answers are the same, then there is no point in asking the question.\n",
    "            if yes_answer == no_answer:\n",
    "                sub_tree = yes_answer\n",
    "            else:\n",
    "                sub_tree[question].append(yes_answer)\n",
    "                sub_tree[question].append(no_answer)\n",
    "            return sub_tree\n",
    "\n",
    "    def predict_example(self,example, tree): ##Prediction that goes down a tree\n",
    "        # print(tree)\n",
    "        question = list(tree.keys())[0]\n",
    "        feature_name, comparison_operator, value = question.split(\" \")\n",
    "        # splitting queistion\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "        # return if it is already not a dictionary (which means it reaches the end)\n",
    "        if type(answer) != dict:\n",
    "            return answer\n",
    "        # recursion\n",
    "        else:\n",
    "            residual_tree = answer\n",
    "            return self.predict_example(example, residual_tree)\n",
    "\n",
    "    # All examples of the test data\n",
    "    def decision_tree_predictions(self,test_df, tree):\n",
    "        predictions = test_df.apply(self.predict_example, args=(tree,), axis=1)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use our adjusted Random Forest algorithm for prediction\n",
    "'''\n",
    "#set the dataFrame to predict comment_num\n",
    "data = pd.read_csv('dataclustered.csv')\n",
    "df=data.drop(['Unnamed: 0','id','comment_num','repost_num','like_num','created_at','image_url','video_url','origin_weibo','content','user_id','time','year'],axis=1)\n",
    "bins = [-1, 50, 100, 1000, 136265]\n",
    "data['comment'] = pd.cut(data['comment_num'], bins,labels=[1,2,3,4],right=True)\n",
    "# Build random forest class\n",
    "My_Random_Forest = RandomForest(df, data['comment'], n_trees=200,n_bootstrap=100, n_features=6, dt_max_depth=10, min_samples=2)\n",
    "predictions = My_Random_Forest.random_forest_predictions()\n",
    "accuracy = calculate_accuracy(predictions,My_Random_Forest.test_df.label)\n",
    "print(accuracy)\n",
    "\n",
    "#set the dataFrame to predict like_num\n",
    "data = pd.read_csv('dataclustered.csv')\n",
    "df=data.drop(['Unnamed: 0','id','comment_num','repost_num','like_num','created_at','image_url','video_url','origin_weibo','content','user_id','time','year'],axis=1)\n",
    "bins = [-1,75 , 500, 1000, 306161]\n",
    "data['like'] = pd.cut(data['like_num'], bins,labels=[1,2,3,4],right=True)\n",
    "# Build random forest class\n",
    "My_Random_Forest = RandomForest(df, data['like'], n_trees=200,n_bootstrap=100, n_features=6, dt_max_depth=10, min_samples=2)\n",
    "predictions = My_Random_Forest.random_forest_predictions()\n",
    "accuracy = calculate_accuracy(predictions,My_Random_Forest.test_df.label)\n",
    "print(accuracy)\n",
    "\n",
    "#set the dataFrame to predict repost_num\n",
    "data = pd.read_csv('dataclustered.csv')\n",
    "df=data.drop(['Unnamed: 0','id','comment_num','repost_num','like_num','created_at','image_url','video_url','origin_weibo','content','user_id','time','year'],axis=1)\n",
    "bins =  [-1, 100, 500, 1000,  565454]\n",
    "data['repost'] = pd.cut(data['repost_num'], bins,labels=[1,2,3,4],right=True)\n",
    "# Build random forest class\n",
    "My_Random_Forest = RandomForest(df, data['repost'], n_trees=200,n_bootstrap=100, n_features=6, dt_max_depth=10, min_samples=2)\n",
    "predictions = My_Random_Forest.random_forest_predictions()\n",
    "accuracy = calculate_accuracy(predictions,My_Random_Forest.test_df.label)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find and visualize the most important 5 features for the prediction model\n",
    "'''\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('dataclustered.csv')\n",
    "data=df.drop(['Unnamed: 0','id','comment_num','repost_num','like_num','type','created_at','image_url','video_url','origin_weibo','content','user_id','time','year'],axis=1)\n",
    "bins = [-1, 50, 100, 1000, 136265]\n",
    "df['comment_num'] = pd.cut(df['comment_num'], bins,labels=[1,2,3,4],right=True)\n",
    "X = data\n",
    "bins = [-1, 100, 500, 1000,  565454]\n",
    "df['repost_num'] = pd.cut(df['repost_num'], bins,labels=[1,2,3,4],right=True)\n",
    "bins = [-1,75 , 500, 1000, 306161]\n",
    "df['like_num'] = pd.cut(df['like_num'], bins,labels=[1,2,3,4],right=True)\n",
    "print(X)\n",
    "#y = df['comment_num']\n",
    "#y = df['repost_num']\n",
    "#y = df['like_num']\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=100,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "df = pd.read_csv('dataclustered.csv')\n",
    "df = df.drop(['Unnamed: 0','id'],axis=1)\n",
    "df[df['type']==9].describe()\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "%matplotlib inline\n",
    "map_vir = cm.get_cmap(name='Greens')\n",
    "n=5\n",
    "norm = plt.Normalize(importances[indices][:n].min(), importances[indices][:n].max())\n",
    "norm_y = norm(importances[indices][:n])\n",
    "color = map_vir(norm_y)\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances for like number\")\n",
    "#plt.title(\"Feature importances for comment number\")\n",
    "#plt.title(\"Feature importances for repost number\")\n",
    "\n",
    "plt.bar(range(5), importances[indices][:5],\n",
    "       color=color, yerr=std[indices][:5], align=\"center\")\n",
    "#plt.xticks(range(5), ['fans','tweets','hash','at','vip'])\n",
    "#plt.xticks(range(5), ['fans','tweets','2013','hash','2012'])\n",
    "#plt.xticks(range(5), ['fans','tweets','hash','at','vip'])\n",
    "#plt.xticks(range(5), indices)\n",
    "plt.xlim([-1, 5])\n",
    "plt.show();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
